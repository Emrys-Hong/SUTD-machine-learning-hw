{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Hong Pengfei id: 1002949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_train = './libsvm-3.24/svm-train'\n",
    "libsvm_pred = './libsvm-3.24/svm-predict'\n",
    "data_train = 'HW3_data/1/promoters/training.txt'\n",
    "data_test = 'HW3_data/1/promoters/test.txt'\n",
    "model = 'output/training.txt.model'\n",
    "output_file = 'output/out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....*...*\n",
      "optimization finished, #iter = 637\n",
      "nu = 0.024407\n",
      "obj = -0.903163, rho = 1.532559\n",
      "nSV = 39, nBSV = 0\n",
      "Total nSV = 39\n",
      "Accuracy = 84.375% (27/32) (classification)\n"
     ]
    }
   ],
   "source": [
    "! {libsvm_train} -t 0 {data_train} {model}\n",
    "! {libsvm_pred} {data_test} {model} {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel type 0 -- linear: u'*v have acc of 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*.*\n",
      "optimization finished, #iter = 186\n",
      "nu = 0.026157\n",
      "obj = -0.967852, rho = 0.276119\n",
      "nSV = 62, nBSV = 0\n",
      "Total nSV = 62\n",
      "Accuracy = 81.25% (26/32) (classification)\n"
     ]
    }
   ],
   "source": [
    "! {libsvm_train} -t 1 {data_train} {model}\n",
    "! {libsvm_pred} {data_test} {model} {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel type 1 -- polynomial: (gamma*u'*v + coef0)^degree has acc of 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*\n",
      "optimization finished, #iter = 96\n",
      "nu = 0.824736\n",
      "obj = -32.541976, rho = -0.133095\n",
      "nSV = 74, nBSV = 25\n",
      "Total nSV = 74\n",
      "Accuracy = 90.625% (29/32) (classification)\n"
     ]
    }
   ],
   "source": [
    "! {libsvm_train} -t 2 {data_train} {model}\n",
    "! {libsvm_pred} {data_test} {model} {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel type 2 -- radial basis function: exp(-gamma*|u-v|^2) has acc of 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "optimization finished, #iter = 39\n",
      "nu = 0.945946\n",
      "obj = -67.705765, rho = -0.697125\n",
      "nSV = 71, nBSV = 69\n",
      "Total nSV = 71\n",
      "Accuracy = 43.75% (14/32) (classification)\n"
     ]
    }
   ],
   "source": [
    "! {libsvm_train} -t 3 {data_train} {model}\n",
    "! {libsvm_pred} {data_test} {model} {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel type of 3 -- sigmoid: tanh(gamma*u'*v + coef0) has acc of 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "### the second kind of kernel has better accuracy on the test set of 90.625%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem is: \\\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i=1}^N \\alpha^i \\\\\n",
    " = \\frac{1}{2} (2 \\alpha^2_1 + \\alpha_2^2 - 2 \\alpha_1 \\alpha_2) - \\alpha_1 - \\alpha_2 \\\\\n",
    " s.t. \\alpha_i \\geq 0  \\qquad i=1, 2\n",
    "\\end{equation}\n",
    "Therefore, we need to\n",
    "\\begin{equation}\n",
    "min_\\alpha \\qquad \\alpha_1^2 + \\frac{1}{2}\\alpha_2^2 - \\alpha_1\\alpha_2 - \\alpha_1 - \\alpha_2\n",
    "\\end{equation}\n",
    "by taking partial derivative we get $\\alpha_1 = 2, \\alpha_2 = 3$ which satisfy the condition $\\alpha_{1,2} > 0$\\\n",
    "Therefore, $ w^* = \\sum \\alpha_i y_i x_i = (-1, 2)$ and margin is $\\gamma = \\frac{1}{||w||} = \\frac{1}{\\sqrt{5}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem is: \\\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i=1}^N \\alpha^i \\\\\n",
    " = \\frac{1}{2} (2 \\alpha^2_1 + \\alpha_2^2 - 2 \\alpha_1 \\alpha_2) - \\alpha_1 - \\alpha_2 \\\\\n",
    " s.t. \\qquad \\alpha_i \\geq 0  \\qquad i=1, 2 \\\\\n",
    " \\qquad \\alpha_1 - \\alpha_2 = 0\n",
    "\\end{equation}\n",
    "Therefore, we need to\n",
    "\\begin{equation}\n",
    "min_\\alpha \\qquad \\frac{1}{2}\\alpha_1^2 - 2 \\alpha_1 \n",
    "\\end{equation}\n",
    "by taking partial derivative we get $\\alpha_1 = \\alpha_2 = 2$ which satisfy the condition $\\alpha_{1,2} > 0$\n",
    "\n",
    "Therefore, $ w^* = \\sum \\alpha_i y_i x_i = (0, 2) $, \n",
    "\n",
    "for $\\alpha_j > 0$ $b^* = y_i - \\sum_{i=1}^{N} \\alpha^* y_i (x_i*x_j) = -1$ \n",
    "\n",
    "margin is $\\gamma = \\frac{1}{||w||} = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. $K(x, z) = K_1(x, z) K_2(x, z)$  is a kernel function\n",
    "\n",
    "\n",
    "### Proof (using Mercer's Thereom):\n",
    "According to Mercer's Thereom, to prove $K(x, z)$ is kernel method is equal to prove its `Gram matrix` A is symmetric and positive semidefinitive.\n",
    "\n",
    "If $K_1(x,z), K_2(x,z)$ are both Kernel func, their `Gram matrix` are symmetric and positive semidefinitive.\\\n",
    "\n",
    "let $A_1$, $A_2$ be the kernel matrix of $K_1(x,z)$, $K_2(x,z)$, $\\therefore$ we need to prove $A = A_1A_2$ is also positive semidefinite\n",
    "\n",
    "$\\because A_1$ is positive semidefinite, $\\therefore A_1 $ can be decomposed into  $A_1 = (Q_1\\Lambda_1^{\\frac{1}{2}})(Q_1\\Lambda_1^{\\frac{1}{2}})^T = VV^T$\n",
    "\n",
    "\n",
    "$\\because \\forall h \\in R_n, \\qquad hVBV^Th^T = (hV)A_2(hV)^T > 0$, $\\therefore VA_2V^T$ is positive semidefinitive  .\\\n",
    "\n",
    "$\\because det(VA_2V^T−𝜆𝐼) = det(VV^TA_2−𝜆𝐼)$ $\\therefore VA_2V^T$ has the same positive eigenavalues as $VV^TA_2$, $\\therefore A = A_1A_2$ is also positive semidefinite.\n",
    "\n",
    "\n",
    "### Proof 2 (using kernel definition):\n",
    "\\begin{align}\n",
    "k_1(x,y) = a(x)^T a(y), \\qquad a( z ) = [a_1(z), a_2(z), \\ldots a_M(z)] \\\\\n",
    "k_2(x,y) = b(x)^T b(y), \\qquad b( z ) = [b_1(z), b_2(z), \\ldots b_N(z)] \n",
    "\\end{align}\n",
    "\n",
    "So $a$ is a function that produces an $M$-dim vector, and $b$ produces an $N$-dim vector.\n",
    "\n",
    "Next, we just write the product in terms of $a$ and $b$, and perform some regrouping.\n",
    "\n",
    "\\begin{align}\n",
    "k_{p}(x,y) &= k_1(x,y) k_2(x,y)\n",
    "\\\\&= \\Big( \\sum_{m=1}^M a_m(x) a_m(y) \\Big) \\Big(  \\sum_{n=1}^N b_n(x) b_n(y) \\Big)\n",
    "\\\\&= \\sum_{m=1}^M \\sum_{n=1}^N [ a_m(x)  b_n(x) ] [a_m(y) b_n(y)]\n",
    "\\\\&= \\sum_{m=1}^M \\sum_{n=1}^N  c_{mn}( x )  c_{mn}( y )\n",
    "\\\\&= c(x)^T c(y)\n",
    "\\end{align}\n",
    "\n",
    "where $c(z)$ is an $M \\cdot N$ -dimensional vector, s.t. $c_{mn}(z) = a_m(z) b_n(z)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. $ K(x, z) = aK_1(x, z) + bK_2(x, z)  \\qquad a, b > 0 \\qquad a, b \\in R$, $K(x,z)$ is Kernel function\n",
    "### Proof (using Mercer's Theorem):\n",
    "According to Mercer's Thereom, to prove $K(x, z)$ is kernel method is equal to prove its `Gram matrix` A is symmetric and positive semidefinitive.\n",
    "\n",
    "If $K_1(x,z), K_2(x,z)$ are both Kernel func, their `Gram matrix` are symmetric and positive semidefinitive.\n",
    "\n",
    "\n",
    "1. To prove $ aK_1(x, z) + bK_2(x, z) $ is symmetric: \\\n",
    "for any matrix M, for any constant $k \\in $, $M * k$ is also a symmetric matrix. $\\because$ $A_{ij} = A_{ji}$ in $M$, $\\therefore$,  $kA_{ij} = kA_{ji}$. Therefore, $aK_1(x,z), bK_2(x,z)$ are all symmetric.\\\n",
    "$\\because$ $ (A+B)^T = A^T + B^T $, and if $A^T = A, B^T = B$, $\\therefore (A+B)^T = A+B$\\\n",
    "$\\because$ $aK_1(x,z), bK_2(x,z)$ are all symmetric, $\\therefore K(x, z)$ is symmetric.\n",
    "\n",
    "\n",
    "\n",
    "2. To prove $ aK_1(x, z) + bK_2(x, z) $ is positive semidefinite:\\\n",
    "Now $K_1(x, z)$ and $K_2(x, z)$ are positive definite matrices, means for all $h \\in R_n$ we must have $hK_1h \\gt 0$ and $hK_2h \\geq 0$. $\\because a, b \\gt 0 $ $\\therefore$ $aK_1(x, z)$ and $bK_2(x,z)$ are all positive semidefinite matrices.\\\n",
    "\n",
    "$\\because$ $0 \\lt haK_1(x, z)h^T + hbK_2(x, z)h^T = h(K_1(x,z)+K_2(x,z))h^T$, $\\therefore$ $ aK_1(x, z) + bK_2(x, z) $ is positive semidefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. $ K(x, z) = aK_1(x, z) - bK_2(x, z)  \\qquad a, b > 0 \\qquad a, b \\in R$, $K(x,z)$ is not a kernel function\n",
    "### Counter example:\n",
    "\n",
    "let $a = 1, b = 2, K_1 = K2$, $\\therefore K(x,z) = -K_2(x,z)$.\n",
    "\n",
    "By Mercer's Theorem, -K_2(x,z) cannot be kernel function because it has a semi-negative kernel matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. $K(x, z) = f(x)f(z),$ where f : Rn -> R be any real valued function of x. This is a kernel.\n",
    "The proof is trivial: \n",
    "\n",
    "$\\because$ by kernel defition $K(x,z) = \\phi(x)\\phi(z)$ where $\\phi(y): Rd \\rightarrow Rp$, in case of p=1, $K(x,z) = f(x)f(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 359 µs, sys: 0 ns, total: 359 µs\n",
      "Wall time: 392 µs\n",
      "3.8429624470446271242e-422\n",
      "\n",
      "CPU times: user 338 µs, sys: 0 ns, total: 338 µs\n",
      "Wall time: 343 µs\n",
      "2.2051243729623736413e-415\n"
     ]
    }
   ],
   "source": [
    "res = 1\n",
    "temp = np.random.rand(1000).astype(np.float128)\n",
    "%time for i in temp: res *= i\n",
    "print(res)\n",
    "print()\n",
    "res = 0\n",
    "temp = np.log(np.random.rand(1000))\n",
    "%time for i in temp: res += i\n",
    "print(np.exp(res.astype(np.float128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using log has certain advantages\n",
    "1. loglikelihood can express a series of multiplication in terms of addition, which is more computationally cheaper. Therefore it only takes more time to calculate multiplication as showed above. \n",
    "2. multiplication of a sequence of numbers that is smaller than 1 will stackoverflow and become zero, therefore needs higher precision like float128, however, it only needs float32 to calculate the equation under the log to a certain amount of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('HW3_data/4/diabetes_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = diabetes.drop(columns=0), diabetes.loc[:, 0][:,np.newaxis]\n",
    "x['bias'] = np.ones((len(x), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## formulas\n",
    "\n",
    "loss:\\\n",
    "\\begin{equation}\n",
    "E = \\sum_{i=1}^{n}\\log{(  1 + \\exp \\{-y^{i} (\\theta \\cdot x^{i} + \\theta_0) \\}    )} \\\n",
    "\\end{equation}\n",
    "\n",
    "derivative of loss: \\\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{E^{(t)}}}{\\partial{\\theta}} =  \\frac{-y^{(t)} x^{(t)}}{1 + exp(y^{(t)} (\\theta x^{(t)}) )}  \\\n",
    "\\end{equation}\n",
    "\n",
    "loglikelihood = -E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(theta, x, y):\n",
    "    # x (dim,)\n",
    "    # y scalar\n",
    "    gradient = - (y * x) / (1 + np.exp(y * (x.dot(theta) ) ) ) \n",
    "    gradient = gradient[:,np.newaxis]\n",
    "    return gradient\n",
    "\n",
    "def SGD(x, y, theta, lr=0.1):\n",
    "    p = np.random.permutation(len(y))\n",
    "    x, y = x.loc[p, :], y[p]\n",
    "    for xi, yi in zip(x.values, y):\n",
    "        gradient = backward(theta, xi, yi)\n",
    "        theta -= lr*gradient\n",
    "    return theta\n",
    "\n",
    "def loss_func(x, y, theta, epoch):\n",
    "    # x (n, dim)\n",
    "    # y (n, 1)\n",
    "    loss = np.mean(np.log( 1 + np.exp( np.multiply(-y, (x.values.dot(theta))))) )\n",
    "    print(f'epoch {epoch} \\t loss {loss}, \\t accuracy is {np.sum(np.sign(x.values.dot(theta)) == y)/ len(y)}')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f10b2856064fea9e82f8aa3b32a532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \t loss 0.22424183521874158, \t accuracy is 0.9356666666666666\n",
      "epoch 100 \t loss 0.014761623415482835, \t accuracy is 0.9986666666666667\n",
      "epoch 200 \t loss 0.012111853320110944, \t accuracy is 0.9976666666666667\n",
      "epoch 300 \t loss 0.010124720225913328, \t accuracy is 0.9983333333333333\n",
      "epoch 400 \t loss 0.008703752531464047, \t accuracy is 0.9996666666666667\n",
      "epoch 500 \t loss 0.008270980601047662, \t accuracy is 0.9996666666666667\n",
      "epoch 600 \t loss 0.007948551094860031, \t accuracy is 0.9983333333333333\n",
      "epoch 700 \t loss 0.006958411690223807, \t accuracy is 0.9993333333333333\n",
      "epoch 800 \t loss 0.006525542073253271, \t accuracy is 1.0\n",
      "epoch 900 \t loss 0.006126002047713899, \t accuracy is 1.0\n",
      "epoch 1000 \t loss 0.0058412673519075186, \t accuracy is 1.0\n",
      "epoch 1100 \t loss 0.005593978816039055, \t accuracy is 1.0\n",
      "epoch 1200 \t loss 0.005374554318872472, \t accuracy is 1.0\n",
      "epoch 1300 \t loss 0.005225690517720888, \t accuracy is 1.0\n",
      "epoch 1400 \t loss 0.0050508446627849, \t accuracy is 1.0\n",
      "epoch 1500 \t loss 0.004869285760506463, \t accuracy is 1.0\n",
      "epoch 1600 \t loss 0.004708346824406036, \t accuracy is 1.0\n",
      "epoch 1700 \t loss 0.004567669550731825, \t accuracy is 1.0\n",
      "epoch 1800 \t loss 0.004485348411101705, \t accuracy is 1.0\n",
      "epoch 1900 \t loss 0.004328536103642741, \t accuracy is 1.0\n",
      "epoch 2000 \t loss 0.004275228388883553, \t accuracy is 1.0\n",
      "epoch 2100 \t loss 0.0041330355171619445, \t accuracy is 1.0\n",
      "epoch 2200 \t loss 0.004092549313256294, \t accuracy is 1.0\n",
      "epoch 2300 \t loss 0.0039562298506813755, \t accuracy is 1.0\n",
      "epoch 2400 \t loss 0.003871029514573084, \t accuracy is 1.0\n",
      "epoch 2500 \t loss 0.0037913728908620273, \t accuracy is 1.0\n",
      "epoch 2600 \t loss 0.003821653336922587, \t accuracy is 1.0\n",
      "epoch 2700 \t loss 0.003684812943944932, \t accuracy is 1.0\n",
      "epoch 2800 \t loss 0.0037472618029406562, \t accuracy is 1.0\n",
      "epoch 2900 \t loss 0.003528616516402073, \t accuracy is 1.0\n",
      "epoch 3000 \t loss 0.0035690976114474238, \t accuracy is 1.0\n",
      "epoch 3100 \t loss 0.003442981962472942, \t accuracy is 1.0\n",
      "epoch 3200 \t loss 0.0033736401480398727, \t accuracy is 1.0\n",
      "epoch 3300 \t loss 0.003317679611192703, \t accuracy is 1.0\n",
      "epoch 3400 \t loss 0.0032672755896274333, \t accuracy is 1.0\n",
      "epoch 3500 \t loss 0.0032094950586838613, \t accuracy is 1.0\n",
      "epoch 3600 \t loss 0.003164982061999034, \t accuracy is 1.0\n",
      "epoch 3700 \t loss 0.0031512616123070217, \t accuracy is 1.0\n",
      "epoch 3800 \t loss 0.0030807724531533836, \t accuracy is 1.0\n",
      "epoch 3900 \t loss 0.0030565798318783464, \t accuracy is 1.0\n",
      "epoch 4000 \t loss 0.002999837682282255, \t accuracy is 1.0\n",
      "epoch 4100 \t loss 0.0030587837856186355, \t accuracy is 1.0\n",
      "epoch 4200 \t loss 0.0029354459650644646, \t accuracy is 1.0\n",
      "epoch 4300 \t loss 0.0029125316486890334, \t accuracy is 1.0\n",
      "epoch 4400 \t loss 0.00288877466970553, \t accuracy is 1.0\n",
      "epoch 4500 \t loss 0.002830205834538726, \t accuracy is 1.0\n",
      "epoch 4600 \t loss 0.0028003810149548397, \t accuracy is 1.0\n",
      "epoch 4700 \t loss 0.002764104752976942, \t accuracy is 1.0\n",
      "epoch 4800 \t loss 0.002870597014113668, \t accuracy is 1.0\n",
      "epoch 4900 \t loss 0.002750636904633829, \t accuracy is 1.0\n",
      "epoch 5000 \t loss 0.0026887316183338943, \t accuracy is 1.0\n",
      "epoch 5100 \t loss 0.002695710682485784, \t accuracy is 1.0\n",
      "epoch 5200 \t loss 0.002623879243710281, \t accuracy is 1.0\n",
      "epoch 5300 \t loss 0.0026058353689982386, \t accuracy is 1.0\n",
      "epoch 5400 \t loss 0.0026372561840874303, \t accuracy is 1.0\n",
      "epoch 5500 \t loss 0.0025753678379592426, \t accuracy is 1.0\n",
      "epoch 5600 \t loss 0.0025302222701247654, \t accuracy is 1.0\n",
      "epoch 5700 \t loss 0.0025003918421673097, \t accuracy is 1.0\n",
      "epoch 5800 \t loss 0.002480096435827171, \t accuracy is 1.0\n",
      "epoch 5900 \t loss 0.0025134036603131345, \t accuracy is 1.0\n",
      "epoch 6000 \t loss 0.002434466271899361, \t accuracy is 1.0\n",
      "epoch 6100 \t loss 0.002413951658974544, \t accuracy is 1.0\n",
      "epoch 6200 \t loss 0.0024209840739165167, \t accuracy is 1.0\n",
      "epoch 6300 \t loss 0.0023907294755423467, \t accuracy is 1.0\n",
      "epoch 6400 \t loss 0.0023619151515384545, \t accuracy is 1.0\n",
      "epoch 6500 \t loss 0.002352686026140898, \t accuracy is 1.0\n",
      "epoch 6600 \t loss 0.0023178265808205594, \t accuracy is 1.0\n",
      "epoch 6700 \t loss 0.0023062014951012377, \t accuracy is 1.0\n",
      "epoch 6800 \t loss 0.0023240994088670826, \t accuracy is 1.0\n",
      "epoch 6900 \t loss 0.002276499358568774, \t accuracy is 1.0\n",
      "epoch 7000 \t loss 0.0022682512787668015, \t accuracy is 1.0\n",
      "epoch 7100 \t loss 0.0022270710758277583, \t accuracy is 1.0\n",
      "epoch 7200 \t loss 0.0022125927296454085, \t accuracy is 1.0\n",
      "epoch 7300 \t loss 0.002196083417208949, \t accuracy is 1.0\n",
      "epoch 7400 \t loss 0.0021784008323006425, \t accuracy is 1.0\n",
      "epoch 7500 \t loss 0.0022091684390285176, \t accuracy is 1.0\n",
      "epoch 7600 \t loss 0.002148889838157114, \t accuracy is 1.0\n",
      "epoch 7700 \t loss 0.0021396814832160232, \t accuracy is 1.0\n",
      "epoch 7800 \t loss 0.002125394857247129, \t accuracy is 1.0\n",
      "epoch 7900 \t loss 0.0021095332814907424, \t accuracy is 1.0\n",
      "epoch 8000 \t loss 0.002090876226809599, \t accuracy is 1.0\n",
      "epoch 8100 \t loss 0.0020780931591603854, \t accuracy is 1.0\n",
      "epoch 8200 \t loss 0.002076078122080182, \t accuracy is 1.0\n",
      "epoch 8300 \t loss 0.0020482269417735643, \t accuracy is 1.0\n",
      "epoch 8400 \t loss 0.0020354770913097666, \t accuracy is 1.0\n",
      "epoch 8500 \t loss 0.002036101246994818, \t accuracy is 1.0\n",
      "epoch 8600 \t loss 0.0020360907587568767, \t accuracy is 1.0\n",
      "epoch 8700 \t loss 0.0019997912462274454, \t accuracy is 1.0\n",
      "epoch 8800 \t loss 0.0019962398878239575, \t accuracy is 1.0\n",
      "epoch 8900 \t loss 0.001972752336958787, \t accuracy is 1.0\n",
      "epoch 9000 \t loss 0.0019626309682399543, \t accuracy is 1.0\n",
      "epoch 9100 \t loss 0.00195947870148912, \t accuracy is 1.0\n",
      "epoch 9200 \t loss 0.0019422648481122722, \t accuracy is 1.0\n",
      "epoch 9300 \t loss 0.0019275202213850351, \t accuracy is 1.0\n",
      "epoch 9400 \t loss 0.001923781719912876, \t accuracy is 1.0\n",
      "epoch 9500 \t loss 0.0019242991851045627, \t accuracy is 1.0\n",
      "epoch 9600 \t loss 0.0018932809163870851, \t accuracy is 1.0\n",
      "epoch 9700 \t loss 0.0018871039622432076, \t accuracy is 1.0\n",
      "epoch 9800 \t loss 0.0018941923145789233, \t accuracy is 1.0\n",
      "epoch 9900 \t loss 0.0018741682784809341, \t accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "lr = 0.1\n",
    "theta = np.random.normal(x.shape[1], 1)\n",
    "epochs = range(10_000)\n",
    "weights = {}\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "for epoch in tqdm_notebook(epochs, file=sys.stdout):\n",
    "    theta = SGD(x, y, theta, lr)\n",
    "    if epoch % 100 == 0: \n",
    "        loss = loss_func(x, y, theta, epoch)\n",
    "        weights[epoch] = theta\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0, 10000, 100), -1*np.array(losses))\n",
    "plt.ylabel('loglikelihood')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('weights.pk', 'wb') as f: pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://math.stackexchange.com/questions/113842/is-the-product-of-symmetric-positive-semidefinite-matrices-positive-definite\n",
    "\n",
    "https://www.cse.iitk.ac.in/users/rmittal/prev_course/s14/notes/lec11.pdf\n",
    "\n",
    "https://stats.stackexchange.com/questions/48509/proof-of-closeness-of-kernel-functions-under-pointwise-product\n",
    "\n",
    "https://ttic.uchicago.edu/~dmcallester/ttic101-07/lectures/kernels/kernels.pdf\n",
    "\n",
    "https://www.cs.cmu.edu/~aarti/Class/10701_Spring14/KernelTheory.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
