{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Hong Pengfei id: 1002949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm_train = './libsvm-3.24/svm-train'\n",
    "libsvm_pred = './libsvm-3.24/svm-predict'\n",
    "data_train = 'HW3_data/1/promoters/training.txt'\n",
    "data_test = 'HW3_data/1/promoters/test.txt'\n",
    "model = 'output/training.txt.model'\n",
    "output_file = 'output/out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....*...*\n",
      "optimization finished, #iter = 637\n",
      "nu = 0.024407\n",
      "obj = -0.903163, rho = 1.532559\n",
      "nSV = 39, nBSV = 0\n",
      "Total nSV = 39\n",
      "Accuracy = 84.375% (27/32) (classification)\n"
     ]
    }
   ],
   "source": [
    "! {libsvm_train} -t 0 {data_train} {model}\n",
    "! {libsvm_pred} {data_test} {model} {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel type 0 -- linear: u'*v have acc of 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*.*\n",
      "optimization finished, #iter = 186\n",
      "nu = 0.026157\n",
      "obj = -0.967852, rho = 0.276119\n",
      "nSV = 62, nBSV = 0\n",
      "Total nSV = 62\n",
      "Accuracy = 81.25% (26/32) (classification)\n"
     ]
    }
   ],
   "source": [
    "! {libsvm_train} -t 1 {data_train} {model}\n",
    "! {libsvm_pred} {data_test} {model} {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel type 1 -- polynomial: (gamma*u'*v + coef0)^degree has acc of 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*\n",
      "optimization finished, #iter = 96\n",
      "nu = 0.824736\n",
      "obj = -32.541976, rho = -0.133095\n",
      "nSV = 74, nBSV = 25\n",
      "Total nSV = 74\n",
      "Accuracy = 90.625% (29/32) (classification)\n"
     ]
    }
   ],
   "source": [
    "! {libsvm_train} -t 2 {data_train} {model}\n",
    "! {libsvm_pred} {data_test} {model} {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel type 2 -- radial basis function: exp(-gamma*|u-v|^2) has acc of 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "optimization finished, #iter = 39\n",
      "nu = 0.945946\n",
      "obj = -67.705765, rho = -0.697125\n",
      "nSV = 71, nBSV = 69\n",
      "Total nSV = 71\n",
      "Accuracy = 43.75% (14/32) (classification)\n"
     ]
    }
   ],
   "source": [
    "! {libsvm_train} -t 3 {data_train} {model}\n",
    "! {libsvm_pred} {data_test} {model} {output_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel type of 3 -- sigmoid: tanh(gamma*u'*v + coef0) has acc of 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "### the second kind of kernel has better accuracy on the test set of 90.625%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem is: \\\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i=1}^N \\alpha^i \\\\\n",
    " = \\frac{1}{2} (2 \\alpha^2_1 + \\alpha_2^2 - 2 \\alpha_1 \\alpha_2) - \\alpha_1 - \\alpha_2 \\\\\n",
    " s.t. \\alpha_i \\geq 0  \\qquad i=1, 2\n",
    "\\end{equation}\n",
    "Therefore, we need to\n",
    "\\begin{equation}\n",
    "min_\\alpha \\qquad \\alpha_1^2 + \\frac{1}{2}\\alpha_2^2 - \\alpha_1\\alpha_2 - \\alpha_1 - \\alpha_2\n",
    "\\end{equation}\n",
    "by taking partial derivative we get $\\alpha_1 = 2, \\alpha_2 = 3$ which satisfy the condition $\\alpha_{1,2} > 0$\\\n",
    "Therefore, $ w^* = \\sum \\alpha_i y_i x_i = (-1, 2)$ and margin is $\\gamma = \\frac{1}{||w||} = \\frac{1}{\\sqrt{5}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem is: \\\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i=1}^N \\alpha^i \\\\\n",
    " = \\frac{1}{2} (2 \\alpha^2_1 + \\alpha_2^2 - 2 \\alpha_1 \\alpha_2) - \\alpha_1 - \\alpha_2 \\\\\n",
    " s.t. \\qquad \\alpha_i \\geq 0  \\qquad i=1, 2 \\\\\n",
    " \\qquad \\alpha_1 - \\alpha_2 = 0\n",
    "\\end{equation}\n",
    "Therefore, we need to\n",
    "\\begin{equation}\n",
    "min_\\alpha \\qquad \\frac{1}{2}\\alpha_1^2 - 2 \\alpha_1 \n",
    "\\end{equation}\n",
    "by taking partial derivative we get $\\alpha_1 = \\alpha_2 = 2$ which satisfy the condition $\\alpha_{1,2} > 0$\n",
    "\n",
    "Therefore, $ w^* = \\sum \\alpha_i y_i x_i = (0, 2) $, \n",
    "\n",
    "for $\\alpha_j > 0$ $b^* = y_i - \\sum_{i=1}^{N} \\alpha^* y_i (x_i*x_j) = -1$ \n",
    "\n",
    "margin is $\\gamma = \\frac{1}{||w||} = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. $K(x, z) = K_1(x, z) K_2(x, z)$  is a kernel function\n",
    "\n",
    "\n",
    "### Proof (using Mercer's Thereom):\n",
    "According to Mercer's Thereom, to prove $K(x, z)$ is kernel method is equal to prove its `Gram matrix` A is symmetric and positive semidefinitive.\n",
    "\n",
    "If $K_1(x,z), K_2(x,z)$ are both Kernel func, their `Gram matrix` are symmetric and positive semidefinitive.\\\n",
    "\n",
    "let $A_1$, $A_2$ be the kernel matrix of $K_1(x,z)$, $K_2(x,z)$, $\\therefore$ we need to prove $A = A_1A_2$ is also positive semidefinite\n",
    "\n",
    "$\\because A_1$ is positive semidefinite, $\\therefore A_1 $ can be decomposed into  $A_1 = (Q_1\\Lambda_1^{\\frac{1}{2}})(Q_1\\Lambda_1^{\\frac{1}{2}})^T = VV^T$\n",
    "\n",
    "\n",
    "$\\because \\forall h \\in R_n, \\qquad hVBV^Th^T = (hV)A_2(hV)^T > 0$, $\\therefore VA_2V^T$ is positive semidefinitive  .\\\n",
    "\n",
    "$\\because det(VA_2V^Tâˆ’ðœ†ð¼) = det(VV^TA_2âˆ’ðœ†ð¼)$ $\\therefore VA_2V^T$ has the same positive eigenavalues as $VV^TA_2$, $\\therefore A = A_1A_2$ is also positive semidefinite.\n",
    "\n",
    "\n",
    "### Proof 2 (using kernel definition):\n",
    "\\begin{align}\n",
    "k_1(x,y) = a(x)^T a(y), \\qquad a( z ) = [a_1(z), a_2(z), \\ldots a_M(z)] \\\\\n",
    "k_2(x,y) = b(x)^T b(y), \\qquad b( z ) = [b_1(z), b_2(z), \\ldots b_N(z)] \n",
    "\\end{align}\n",
    "\n",
    "So $a$ is a function that produces an $M$-dim vector, and $b$ produces an $N$-dim vector.\n",
    "\n",
    "Next, we just write the product in terms of $a$ and $b$, and perform some regrouping.\n",
    "\n",
    "\\begin{align}\n",
    "k_{p}(x,y) &= k_1(x,y) k_2(x,y)\n",
    "\\\\&= \\Big( \\sum_{m=1}^M a_m(x) a_m(y) \\Big) \\Big(  \\sum_{n=1}^N b_n(x) b_n(y) \\Big)\n",
    "\\\\&= \\sum_{m=1}^M \\sum_{n=1}^N [ a_m(x)  b_n(x) ] [a_m(y) b_n(y)]\n",
    "\\\\&= \\sum_{m=1}^M \\sum_{n=1}^N  c_{mn}( x )  c_{mn}( y )\n",
    "\\\\&= c(x)^T c(y)\n",
    "\\end{align}\n",
    "\n",
    "where $c(z)$ is an $M \\cdot N$ -dimensional vector, s.t. $c_{mn}(z) = a_m(z) b_n(z)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. $ K(x, z) = aK_1(x, z) + bK_2(x, z)  \\qquad a, b > 0 \\qquad a, b \\in R$, $K(x,z)$ is Kernel function\n",
    "### Proof (using Mercer's Theorem):\n",
    "According to Mercer's Thereom, to prove $K(x, z)$ is kernel method is equal to prove its `Gram matrix` A is symmetric and positive semidefinitive.\n",
    "\n",
    "If $K_1(x,z), K_2(x,z)$ are both Kernel func, their `Gram matrix` are symmetric and positive semidefinitive.\n",
    "\n",
    "\n",
    "1. To prove $ aK_1(x, z) + bK_2(x, z) $ is symmetric: \\\n",
    "for any matrix M, for any constant $k \\in $, $M * k$ is also a symmetric matrix. $\\because$ $A_{ij} = A_{ji}$ in $M$, $\\therefore$,  $kA_{ij} = kA_{ji}$. Therefore, $aK_1(x,z), bK_2(x,z)$ are all symmetric.\\\n",
    "$\\because$ $ (A+B)^T = A^T + B^T $, and if $A^T = A, B^T = B$, $\\therefore (A+B)^T = A+B$\\\n",
    "$\\because$ $aK_1(x,z), bK_2(x,z)$ are all symmetric, $\\therefore K(x, z)$ is symmetric.\n",
    "\n",
    "\n",
    "\n",
    "2. To prove $ aK_1(x, z) + bK_2(x, z) $ is positive semidefinite:\\\n",
    "Now $K_1(x, z)$ and $K_2(x, z)$ are positive definite matrices, means for all $h \\in R_n$ we must have $hK_1h \\gt 0$ and $hK_2h \\geq 0$. $\\because a, b \\gt 0 $ $\\therefore$ $aK_1(x, z)$ and $bK_2(x,z)$ are all positive semidefinite matrices.\\\n",
    "\n",
    "$\\because$ $0 \\lt haK_1(x, z)h^T + hbK_2(x, z)h^T = h(K_1(x,z)+K_2(x,z))h^T$, $\\therefore$ $ aK_1(x, z) + bK_2(x, z) $ is positive semidefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. $ K(x, z) = aK_1(x, z) - bK_2(x, z)  \\qquad a, b > 0 \\qquad a, b \\in R$, $K(x,z)$ is not a kernel function\n",
    "### Counter example:\n",
    "\n",
    "let $a = 1, b = 2, K_1 = K2$, $\\therefore K(x,z) = -K_2(x,z)$.\n",
    "\n",
    "By Mercer's Theorem, -K_2(x,z) cannot be kernel function because it has a semi-negative kernel matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. $K(x, z) = f(x)f(z),$ where f : Rn -> R be any real valued function of x. This is a kernel.\n",
    "The proof is trivial: \n",
    "\n",
    "$\\because$ by kernel defition $K(x,z) = \\phi(x)\\phi(z)$ where $\\phi(y): Rd \\rightarrow Rp$, in case of p=1, $K(x,z) = f(x)f(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 359 Âµs, sys: 0 ns, total: 359 Âµs\n",
      "Wall time: 392 Âµs\n",
      "3.8429624470446271242e-422\n",
      "\n",
      "CPU times: user 338 Âµs, sys: 0 ns, total: 338 Âµs\n",
      "Wall time: 343 Âµs\n",
      "2.2051243729623736413e-415\n"
     ]
    }
   ],
   "source": [
    "res = 1\n",
    "temp = np.random.rand(1000).astype(np.float128)\n",
    "%time for i in temp: res *= i\n",
    "print(res)\n",
    "print()\n",
    "res = 0\n",
    "temp = np.log(np.random.rand(1000))\n",
    "%time for i in temp: res += i\n",
    "print(np.exp(res.astype(np.float128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using log has certain advantages\n",
    "1. loglikelihood can express a series of multiplication in terms of addition, which is more computationally cheaper. Therefore it only takes more time to calculate multiplication as showed above. \n",
    "2. multiplication of a sequence of numbers that is smaller than 1 will stackoverflow and become zero, therefore needs higher precision like float128, however, it only needs float32 to calculate the equation under the log to a certain amount of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('HW3_data/4/diabetes_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = diabetes.drop(columns=0), diabetes.loc[:, 0]\n",
    "x['bias'] = np.ones((len(x), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## formulas\n",
    "\n",
    "loss:\\\n",
    "\\begin{equation}\n",
    "E = \\sum_{i=1}^{n}\\log{(  1 + \\exp \\{-y^{i} (\\theta \\cdot x^{i} + \\theta_0) \\}    )} \\\n",
    "\\end{equation}\n",
    "\n",
    "derivative of loss: \\\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{E^{(t)}}}{\\partial{\\theta}} =  \\frac{-y^{(t)} x^{(t)}}{1 + exp(y^{(t)} (\\theta x^{(t)}) )}  \\\n",
    "\\end{equation}\n",
    "\n",
    "loglikelihood = -E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(theta, x, y):\n",
    "    gradient = - (y * x) / (1 + np.exp(y * (x.dot(theta) ) ) ) \n",
    "    gradient = gradient[:,np.newaxis]\n",
    "    return gradient\n",
    "\n",
    "def SGD(x, y, theta, lr=0.1):\n",
    "    p = np.random.permutation(len(y))\n",
    "    x, y = x.loc[p, :], y[p]\n",
    "    for xi, yi in zip(x.values, y):\n",
    "        gradient = backward(theta, xi, yi)\n",
    "        theta -= lr*gradient\n",
    "    return theta\n",
    "\n",
    "def loss_func(x, y, theta, epoch):\n",
    "    loss = np.sum(np.log(1+ np.exp( -y.values * (x.values.dot(theta)))))\n",
    "    print(f'epoch {epoch} \\t loss {loss}')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \t loss 7524736.168585648\n",
      "epoch 100 \t loss 7137543.311100427\n",
      "epoch 200 \t loss 6856915.2691986915\n",
      "epoch 300 \t loss 6651448.224132891\n",
      "epoch 400 \t loss 6499385.264668434\n",
      "epoch 500 \t loss 6385571.8880626615\n",
      "epoch 600 \t loss 6299380.385111051\n",
      "epoch 700 \t loss 6233297.501043934\n",
      "epoch 800 \t loss 6181965.938960483\n",
      "epoch 900 \t loss 6141533.475348997\n",
      "epoch 1000 \t loss 6109208.274608007\n",
      "epoch 1100 \t loss 6082950.024858209\n",
      "epoch 1200 \t loss 6061258.141602307\n",
      "epoch 1300 \t loss 6043021.146142089\n",
      "epoch 1400 \t loss 6027411.382771073\n",
      "epoch 1500 \t loss 6013809.276265552\n",
      "epoch 1600 \t loss 6001749.065410418\n",
      "epoch 1700 \t loss 5990879.287030563\n",
      "epoch 1800 \t loss 5980933.942555176\n",
      "epoch 1900 \t loss 5971711.486144179\n",
      "epoch 2000 \t loss 5963058.807772115\n",
      "epoch 2100 \t loss 5954859.577341057\n",
      "epoch 2200 \t loss 5947025.418508004\n",
      "epoch 2300 \t loss 5939489.253573062\n",
      "epoch 2400 \t loss 5932200.267550527\n",
      "epoch 2500 \t loss 5925120.058779978\n",
      "epoch 2600 \t loss 5918219.761250115\n",
      "epoch 2700 \t loss 5911477.737167371\n",
      "epoch 2800 \t loss 5904877.913115018\n",
      "epoch 2900 \t loss 5898408.314843515\n",
      "epoch 3000 \t loss 5892060.284989488\n",
      "epoch 3100 \t loss 5885827.537082434\n",
      "epoch 3200 \t loss 5879705.558446781\n",
      "epoch 3300 \t loss 5873691.255460657\n",
      "epoch 3400 \t loss 5867782.465921839\n",
      "epoch 3500 \t loss 5861977.830506332\n",
      "epoch 3600 \t loss 5856276.397610746\n",
      "epoch 3700 \t loss 5850677.565259066\n",
      "epoch 3800 \t loss 5845181.034633423\n",
      "epoch 3900 \t loss 5839786.651710155\n",
      "epoch 4000 \t loss 5834494.241995273\n",
      "epoch 4100 \t loss 5829303.706191966\n",
      "epoch 4200 \t loss 5824214.971219976\n",
      "epoch 4300 \t loss 5819227.969854242\n",
      "epoch 4400 \t loss 5814342.459628417\n",
      "epoch 4500 \t loss 5809558.22619154\n",
      "epoch 4600 \t loss 5804874.963089282\n",
      "epoch 4700 \t loss 5800292.278578114\n",
      "epoch 4800 \t loss 5795809.715506738\n",
      "epoch 4900 \t loss 5791426.795462063\n",
      "epoch 5000 \t loss 5787143.00128025\n",
      "epoch 5100 \t loss 5782957.70292878\n",
      "epoch 5200 \t loss 5778870.149298544\n",
      "epoch 5300 \t loss 5774879.722454584\n",
      "epoch 5400 \t loss 5770985.6091489475\n",
      "epoch 5500 \t loss 5767187.004857674\n",
      "epoch 5600 \t loss 5763483.09004954\n",
      "epoch 5700 \t loss 5759872.916396614\n",
      "epoch 5800 \t loss 5756355.650031657\n",
      "epoch 5900 \t loss 5752930.386122698\n",
      "epoch 6000 \t loss 5749596.1161600575\n",
      "epoch 6100 \t loss 5746351.908374209\n",
      "epoch 6200 \t loss 5743196.826765937\n",
      "epoch 6300 \t loss 5740129.858476397\n",
      "epoch 6400 \t loss 5737150.067060079\n",
      "epoch 6500 \t loss 5734256.338087892\n",
      "epoch 6600 \t loss 5731447.741458081\n",
      "epoch 6700 \t loss 5728723.279919795\n",
      "epoch 6800 \t loss 5726081.996708922\n",
      "epoch 6900 \t loss 5723522.862328094\n",
      "epoch 7000 \t loss 5721044.854136627\n",
      "epoch 7100 \t loss 5718646.99276162\n",
      "epoch 7200 \t loss 5716328.236601584\n",
      "epoch 7300 \t loss 5714087.71573069\n",
      "epoch 7400 \t loss 5711924.434794932\n",
      "epoch 7500 \t loss 5709837.381849797\n",
      "epoch 7600 \t loss 5707825.564079687\n",
      "epoch 7700 \t loss 5705888.109467826\n",
      "epoch 7800 \t loss 5704024.033271675\n",
      "epoch 7900 \t loss 5702232.403752865\n",
      "epoch 8000 \t loss 5700512.32738063\n",
      "epoch 8100 \t loss 5698862.810633905\n",
      "epoch 8200 \t loss 5697283.010761034\n",
      "epoch 8300 \t loss 5695772.041928323\n",
      "epoch 8400 \t loss 5694329.020211045\n",
      "epoch 8500 \t loss 5692953.041978654\n",
      "epoch 8600 \t loss 5691643.240287148\n",
      "epoch 8700 \t loss 5690398.833596267\n",
      "epoch 8800 \t loss 5689218.871267025\n",
      "epoch 8900 \t loss 5688102.602654359\n",
      "epoch 9000 \t loss 5687049.219503318\n",
      "epoch 9100 \t loss 5686057.87062994\n",
      "epoch 9200 \t loss 5685127.795563191\n",
      "epoch 9300 \t loss 5684258.144447891\n",
      "epoch 9400 \t loss 5683448.187576098\n",
      "epoch 9500 \t loss 5682697.167214409\n",
      "epoch 9600 \t loss 5682004.266795074\n",
      "epoch 9700 \t loss 5681368.797770896\n",
      "epoch 9800 \t loss 5680789.975732582\n",
      "epoch 9900 \t loss 5680267.140645787\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "lr = 0.00000001\n",
    "theta = np.random.randn(x.shape[1], 1)\n",
    "weights = {}\n",
    "for epoch in range(10_000):\n",
    "    theta = SGD(x, y, theta, lr)\n",
    "    if epoch % 100 == 0: \n",
    "        loss = loss_func(x, y, theta, epoch)\n",
    "        weights[epoch] = theta\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.8826666666666667\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy is {np.sum(np.sign(x.values.dot(theta)).squeeze() == y)/ len(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11baa0160>]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0, 10000, 100), -1*np.array(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://math.stackexchange.com/questions/113842/is-the-product-of-symmetric-positive-semidefinite-matrices-positive-definite\n",
    "\n",
    "https://www.cse.iitk.ac.in/users/rmittal/prev_course/s14/notes/lec11.pdf\n",
    "\n",
    "https://stats.stackexchange.com/questions/48509/proof-of-closeness-of-kernel-functions-under-pointwise-product\n",
    "\n",
    "https://ttic.uchicago.edu/~dmcallester/ttic101-07/lectures/kernels/kernels.pdf\n",
    "\n",
    "https://www.cs.cmu.edu/~aarti/Class/10701_Spring14/KernelTheory.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
